{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0f979b-4aa1-4de9-b930-c4dd822fdde5",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Wise Regulatory Data Processor](data/wise_header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a6f3ea-2cb5-4ad9-a726-e91565096ae2",
   "metadata": {},
   "source": [
    "# Wise Regulatory Data Processor\n",
    "\n",
    "## Overview:\n",
    "This notebook is designed to automate the extraction, transformation, and loading (ETL) of regulatory data for Wise, using input from an XLS file. It processes transaction data to meet regulatory reporting requirements for different entities (UK and US), including cross-currency and same-currency volumes. The processed data is uploaded to Google Cloud Storage and subsequently loaded into BigQuery for further analysis and reporting.\n",
    "\n",
    "## Objectives:\n",
    "- Parse the provided XLS file containing transactional data.\n",
    "- Aggregate and transform the data to meet regulatory requirements for UK and US entities.\n",
    "- Upload the transformed data to Google Cloud Storage.\n",
    "- Load the data into BigQuery for further analysis and regulatory reporting.\n",
    "\n",
    "## Key Steps:\n",
    "1. Read and process the XLS file with multiple sheets.\n",
    "2. Transform the data to meet the reporting requirements (R1 & R2).\n",
    "3. Upload the data to Google Cloud Storage.\n",
    "4. Load the transformed data into BigQuery.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb5452-85ad-414a-b4c7-846360cc1967",
   "metadata": {},
   "source": [
    "## Table of Contents <a id='section_0'></a>\n",
    "\n",
    "* [1. Prepare work environment](#section_1)\n",
    "    * [1.1. Library loading](#section_1.1)  \n",
    "    * [1.2. Configuration and Storage Setup](#section_1.2)\n",
    "    * [1.2. load the data](#section_1.2)\n",
    "\n",
    "* [2. Load the data](#section_2)\n",
    "\n",
    "* [3. Data Cleaning](#section_3)\n",
    "\n",
    "* [4. Basic QA Checklist](#section_4)\n",
    "\n",
    "* [5. Save DataFrames](#section_5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85bed4a-6c94-4877-92d8-aa5d43d60054",
   "metadata": {},
   "source": [
    "## 1. Prepare work environment <a id='section_1'></a>\n",
    "\n",
    "* [Table of Contents](#section_0)\n",
    "\n",
    "The work environment is set up by importing the libraries, creating the folders that will be used,\n",
    "setting the permissions and referencing those folders so that it is easy to access.\n",
    "\n",
    "### 1.1. Library loading <a id='section_1.1'></a>\n",
    "* [Table of Contents](#section_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7487eaf5-eca2-4b82-8a35-0e0d06c9e6ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dbba51f-fcac-48ba-8264-00edfa97c922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python standard libraries\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import logging\n",
    "import datetime\n",
    "from typing import Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import openpyxl\n",
    "from openpyxl.styles import Font, Border, Side, Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797c21d-5576-4073-ad7c-6e02f3d36a76",
   "metadata": {},
   "source": [
    "### 1.2. Configuration and Storage Setup <a id='section_1.2'></a>\n",
    "* [Table of Contents](#section_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d30e4103-a909-4bd7-989c-c92293b97d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure Google Cloud Storage client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Define Google Cloud Variables\n",
    "bucket_name = \"wise_csb\" \n",
    "project_id = \"wiseentitydataflow\"\n",
    "dataset_id = \"wise_dataset\" \n",
    "table_id_customer = \"customers\" \n",
    "table_id_transactions = \"transactions\" \n",
    "\n",
    "destination_blob_name_customer = \"customer_data.csv\"\n",
    "destination_blob_name_transactions = \"transactions_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff8125-817e-4b46-aeae-c0f8556fc72f",
   "metadata": {},
   "source": [
    "## 2. Load the data <a id='section_2'></a>\n",
    "\n",
    "* [Table of Contents](#section_0)\n",
    "\n",
    "This code loads data from an Excel file ('Dummy Data.xlsx') containing Customer and Transaction sheets. It processes each sheet into separate DataFrames, stores them in a dictionary with cleaned sheet names as keys, and then creates specific DataFrames for customer and transaction data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a64674a-fddf-4630-9019-fecfb350bd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames 'customer_df' and 'transactions_df' created successfully.\n",
      "\n",
      "First 2 rows of 'customer_df':\n",
      "    Unnamed: 0  Customer_Id  Customer_Type Current_Address_Country  \\\n",
      "0         NaN             1      Personal                     GBR   \n",
      "1         NaN             2      Business                     ESP   \n",
      "\n",
      "  Customer_Since_Date  Unnamed: 5  Unnamed: 6          Field  \\\n",
      "0          2023-02-23         NaN         NaN    Customer_Id   \n",
      "1          2023-10-18         NaN         NaN  Customer_Type   \n",
      "\n",
      "                                          Definition  \n",
      "0            Unique Identifier for a given customer   \n",
      "1  Flag that identifies whether the customer is a...  \n",
      "\n",
      "First 2 rows of 'transactions_df':\n",
      "    Unnamed: 0  Transaction Id  Customer_Id   Amount_GBP Currency_Route   \\\n",
      "0         NaN              73             1         809     GBP --> AUD   \n",
      "1         NaN             102             1         184     EUR --> EUR   \n",
      "\n",
      "  Transaction Date  Unnamed: 6           Field  \\\n",
      "0       2022-08-23         NaN  Transaction_Id   \n",
      "1       2022-02-07         NaN    Customer_Id    \n",
      "\n",
      "                                   Definition  \n",
      "0  Unique Identifier for a given transaction   \n",
      "1     Unique Identifier for a given customer   \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Excel file path (using a variable for easier modification)\n",
    "excel_file_path = \"data/Dummy Data.xlsx\"\n",
    "\n",
    "def load_data(excel_file_path):\n",
    "    \"\"\"\n",
    "    Loads data from an Excel file containing Customer and Transaction sheets.\n",
    "\n",
    "    Args:\n",
    "      excel_file_path: Path to the Excel file.\n",
    "\n",
    "    Returns:\n",
    "      A tuple containing two pandas DataFrames: customer_df and transactions_df.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Excel file and load the sheets into DataFrames\n",
    "        excel_file = pd.ExcelFile(excel_file_path)\n",
    "\n",
    "        # Create a dictionary to store the DataFrames\n",
    "        sheets = {}\n",
    "\n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            df = excel_file.parse(sheet_name, header=1)  # Read with header\n",
    "            # Clean the sheet name of whitespace\n",
    "            cleaned_sheet_name = sheet_name.strip()\n",
    "            sheets[cleaned_sheet_name] = df\n",
    "\n",
    "        # Access the DataFrame of the sheet 'Customer'\n",
    "        customer_df = sheets['Customer']\n",
    "\n",
    "        # Access the DataFrame of the sheet 'Transactions'\n",
    "        transactions_df = sheets['Transactions']\n",
    "\n",
    "        print(\"DataFrames 'customer_df' and 'transactions_df' created successfully.\")\n",
    "\n",
    "        print(\"\\nFirst 2 rows of 'customer_df':\\n\", customer_df.head(2))\n",
    "        print(\"\\nFirst 2 rows of 'transactions_df':\\n\", transactions_df.head(2))\n",
    "\n",
    "        return customer_df, transactions_df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Excel file not found at {excel_file_path}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the Excel file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Run the fucntion to updload data\n",
    "customer_df, transactions_df = load_data(excel_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff43a34-21a8-4987-a5a7-ea0e9268d8ed",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning <a id='section_3'></a>\n",
    "\n",
    "* [Table of Contents](#section_0)\n",
    "\n",
    "\n",
    "This code processes customer_df and transactions_df, by changing their headers to use the first row as column names. It filters both DataFrames to retain only specific columns. Finally, it prints the cleaned and filtered DataFrames for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27353690-1b99-414d-b9f1-45ccb8524a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 20:24:05,771 - INFO - Initial customer_df shape: (100, 9)\n",
      "2024-10-06 20:24:05,772 - INFO - Initial transactions_df shape: (90, 9)\n",
      "2024-10-06 20:24:05,773 - INFO - Preprocessing customer_df...\n",
      "2024-10-06 20:24:05,774 - INFO - Original columns in customer_df: ['Unnamed: 0', 'Customer_Id ', 'Customer_Type', 'Current_Address_Country', 'Customer_Since_Date', 'Unnamed: 5', 'Unnamed: 6', 'Field', 'Definition']\n",
      "2024-10-06 20:24:05,778 - INFO - Columns after initial cleaning in customer_df: ['Customer_Id ', 'Customer_Type', 'Current_Address_Country', 'Customer_Since_Date']\n",
      "2024-10-06 20:24:05,780 - INFO - Preprocessing transactions_df...\n",
      "2024-10-06 20:24:05,781 - INFO - Original columns in transactions_df: ['Unnamed: 0', 'Transaction Id', 'Customer_Id ', 'Amount_GBP', 'Currency_Route ', 'Transaction Date', 'Unnamed: 6', 'Field', 'Definition']\n",
      "2024-10-06 20:24:05,783 - INFO - Columns after initial cleaning in transactions_df: ['Transaction Id', 'Customer_Id ', 'Amount_GBP', 'Currency_Route ', 'Transaction Date']\n",
      "2024-10-06 20:24:05,786 - INFO - Starting data cleaning process...\n",
      "2024-10-06 20:24:05,789 - INFO - Data cleaning completed successfully\n",
      "2024-10-06 20:24:05,790 - INFO - Final customer_df shape: (96, 4)\n",
      "2024-10-06 20:24:05,790 - INFO - Final transactions_df shape: (85, 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns originals in customer_df: ['Unnamed: 0', 'Customer_Id ', 'Customer_Type', 'Current_Address_Country', 'Customer_Since_Date', 'Unnamed: 5', 'Unnamed: 6', 'Field', 'Definition']\n",
      "\n",
      "Columns originals in transactions_df: ['Unnamed: 0', 'Transaction Id', 'Customer_Id ', 'Amount_GBP', 'Currency_Route ', 'Transaction Date', 'Unnamed: 6', 'Field', 'Definition']\n",
      "\n",
      "DataFrames cleaned successfully.\n",
      "\n",
      "First 2 rows of 'customer_df':\n",
      "   Customer_Id Customer_Type Current_Address_Country Customer_Since_Date\n",
      "4            5      Business                     FRA          2023-10-23\n",
      "5            6      Business                     ISL          2022-12-26\n",
      "\n",
      "Customer DataFrame Info:\n",
      "Total customers: 96\n",
      "Customer types distribution:\n",
      "Customer_Type\n",
      "Personal    65\n",
      "Business    27\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 2 rows of 'transactions_df':\n",
      "   Transaction_Id  Customer_Id  Amount_GBP Currency_Route Transaction_Date\n",
      "5              85            3         460        USD-USD       2022-12-30\n",
      "6             117            4        1000        USD-AUD       2022-08-21\n",
      "\n",
      "Transaction DataFrame Info:\n",
      "Total transactions: 85\n",
      "Average transaction amount: £484.89\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class DataFrameColumns:\n",
    "    \"\"\"Define required columns and their mappings\"\"\"\n",
    "    customer_columns = {\n",
    "        # Original column name -> Standardized name\n",
    "        'Customer_Id': 'Customer_Id',  # Exact match from original\n",
    "        'Customer_Type': 'Customer_Type',\n",
    "        'Current_Address_Country': 'Current_Address_Country',\n",
    "        'Customer_Since_Date': 'Customer_Since_Date'\n",
    "    }\n",
    "    \n",
    "    transaction_columns = {\n",
    "        # Original column name -> Standardized name\n",
    "        'Transaction Id': 'Transaction_Id',  # Note the space\n",
    "        'Customer_Id': 'Customer_Id',\n",
    "        'Amount_GBP': 'Amount_GBP',\n",
    "        'Currency_Route': 'Currency_Route',\n",
    "        'Transaction Date': 'Transaction_Date'  # Note the space\n",
    "    }\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame, required_columns: dict, df_name: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Preprocesses DataFrame by removing unnecessary columns and standardizing column names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Preprocessing {df_name}...\")\n",
    "        logger.info(f\"Original columns in {df_name}: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Remove rows that are actually metadata (where Field and Definition columns exist)\n",
    "        if 'Field' in df.columns and 'Definition' in df.columns:\n",
    "            df = df[df['Field'].isna()].copy()\n",
    "        \n",
    "        # Remove unnecessary columns\n",
    "        columns_to_drop = ['Unnamed: 0', 'Unnamed: 5', 'Unnamed: 6', 'Field', 'Definition']\n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "        \n",
    "        # Log available columns after initial cleaning\n",
    "        logger.info(f\"Columns after initial cleaning in {df_name}: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Before checking required columns, let's clean up column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Verify required columns exist\n",
    "        present_columns = set(df.columns)\n",
    "        required_column_names = set(required_columns.keys())\n",
    "        \n",
    "        if not required_column_names.issubset(present_columns):\n",
    "            missing = required_column_names - present_columns\n",
    "            logger.error(f\"Missing required columns in {df_name}: {missing}\")\n",
    "            logger.error(f\"Available columns: {present_columns}\")\n",
    "            return None\n",
    "        \n",
    "        # Select and rename columns\n",
    "        df = df[list(required_columns.keys())].copy()\n",
    "        df = df.rename(columns=required_columns)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preprocessing {df_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def clean_data(\n",
    "    customer_df: pd.DataFrame, \n",
    "    transactions_df: pd.DataFrame\n",
    ") -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses customer and transaction data for regulatory reporting\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Log initial DataFrame shapes\n",
    "        logger.info(f\"Initial customer_df shape: {customer_df.shape}\")\n",
    "        logger.info(f\"Initial transactions_df shape: {transactions_df.shape}\")\n",
    "        \n",
    "        columns = DataFrameColumns()\n",
    "        \n",
    "        # Preprocess DataFrames\n",
    "        customer_df = preprocess_dataframe(customer_df, columns.customer_columns, \"customer_df\")\n",
    "        transactions_df = preprocess_dataframe(transactions_df, columns.transaction_columns, \"transactions_df\")\n",
    "        \n",
    "        if customer_df is None or transactions_df is None:\n",
    "            raise ValueError(\"Preprocessing failed\")\n",
    "\n",
    "        logger.info(\"Starting data cleaning process...\")\n",
    "        \n",
    "        # Clean up dates\n",
    "        customer_df['Customer_Since_Date'] = pd.to_datetime(\n",
    "            customer_df['Customer_Since_Date'],\n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "        transactions_df['Transaction_Date'] = pd.to_datetime(\n",
    "            transactions_df['Transaction_Date'],\n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "        # Clean up Currency_Route (remove arrows and spaces)\n",
    "        transactions_df['Currency_Route'] = transactions_df['Currency_Route'].str.replace(' --> ', '-')\n",
    "        \n",
    "        # Clean up customer type and country\n",
    "        customer_df['Customer_Type'] = customer_df['Customer_Type'].str.strip()\n",
    "        customer_df['Current_Address_Country'] = customer_df['Current_Address_Country'].str.strip()\n",
    "        \n",
    "        # Convert Amount_GBP to numeric, removing any currency symbols if present\n",
    "        transactions_df['Amount_GBP'] = pd.to_numeric(\n",
    "            transactions_df['Amount_GBP'].astype(str).str.replace('[£,]', '', regex=True),\n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Data cleaning completed successfully\")\n",
    "        logger.info(f\"Final customer_df shape: {customer_df.shape}\")\n",
    "        logger.info(f\"Final transactions_df shape: {transactions_df.shape}\")\n",
    "        \n",
    "        return customer_df, transactions_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during data cleaning: {str(e)}\", exc_info=True)\n",
    "        return None, None\n",
    "\n",
    "def display_dataframe_info(customer_df: Optional[pd.DataFrame], \n",
    "                         transactions_df: Optional[pd.DataFrame]) -> None:\n",
    "    \"\"\"\n",
    "    Safely displays information about the cleaned DataFrames\n",
    "    \"\"\"\n",
    "    if customer_df is not None and transactions_df is not None:\n",
    "        print(\"\\nDataFrames cleaned successfully.\")\n",
    "        print(\"\\nFirst 2 rows of 'customer_df':\")\n",
    "        print(customer_df.head(2))\n",
    "        print(\"\\nCustomer DataFrame Info:\")\n",
    "        print(f\"Total customers: {len(customer_df)}\")\n",
    "        print(f\"Customer types distribution:\\n{customer_df['Customer_Type'].value_counts()}\")\n",
    "        \n",
    "        print(\"\\nFirst 2 rows of 'transactions_df':\")\n",
    "        print(transactions_df.head(2))\n",
    "        print(\"\\nTransaction DataFrame Info:\")\n",
    "        print(f\"Total transactions: {len(transactions_df)}\")\n",
    "        print(f\"Average transaction amount: £{transactions_df['Amount_GBP'].mean():.2f}\")\n",
    "    else:\n",
    "        print(\"Data cleaning failed. Please check the logs for details.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Antes de la limpieza, veamos la estructura actual\n",
    "    print(\"\\nColumns originals in customer_df:\", customer_df.columns.tolist())\n",
    "    print(\"\\nColumns originals in transactions_df:\", transactions_df.columns.tolist())\n",
    "    \n",
    "    # Clean data\n",
    "    cleaned_customer_df, cleaned_transactions_df = clean_data(customer_df, transactions_df)\n",
    "    \n",
    "    # Print results\n",
    "    display_dataframe_info(cleaned_customer_df, cleaned_transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94037bbe-c698-4fb2-b219-3c3e7eb37114",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrames cleaned successfully.\n",
      "\n",
      "First 2 rows of 'customer_df':\n",
      "   Customer_Id Customer_Type Current_Address_Country Customer_Since_Date\n",
      "4            5      Business                     FRA          2023-10-23\n",
      "5            6      Business                     ISL          2022-12-26\n",
      "\n",
      "Customer DataFrame Info:\n",
      "Total customers: 96\n",
      "Customer types distribution:\n",
      "Customer_Type\n",
      "Personal    65\n",
      "Business    27\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 2 rows of 'transactions_df':\n",
      "   Transaction_Id  Customer_Id  Amount_GBP Currency_Route Transaction_Date\n",
      "5              85            3         460        USD-USD       2022-12-30\n",
      "6             117            4        1000        USD-AUD       2022-08-21\n",
      "\n",
      "Transaction DataFrame Info:\n",
      "Total transactions: 85\n",
      "Average transaction amount: £484.89\n"
     ]
    }
   ],
   "source": [
    "# Update the original DataFrames with their cleaned versions\n",
    "customer_df = cleaned_customer_df\n",
    "transactions_df = cleaned_transactions_df\n",
    "\n",
    "# Print results\n",
    "display_dataframe_info(customer_df, transactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5b8cd-bc4a-40ec-9f51-fbfc5237d93c",
   "metadata": {},
   "source": [
    "## 4. Basic QA Checklist <a id='section_4'></a>\n",
    "\n",
    "* [Table of Contents](#section_0)\n",
    "\n",
    "Before uploading your DataFrames customer_df and transactions_df to Google Cloud Storage (GCS), it's important to perform a basic quality assurance (QA) check to ensure that the data is correct and in the appropriate format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6278d733-fc32-4bf0-86e7-0c19cc268c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data quality checks...\n",
      "Structure of customer_df:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 96 entries, 4 to 99\n",
      "Data columns (total 4 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   Customer_Id              96 non-null     int64         \n",
      " 1   Customer_Type            92 non-null     object        \n",
      " 2   Current_Address_Country  96 non-null     object        \n",
      " 3   Customer_Since_Date      96 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(1), object(2)\n",
      "memory usage: 3.8+ KB\n",
      "None\n",
      "\n",
      "Structure of transactions_df:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 85 entries, 5 to 89\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   Transaction_Id    85 non-null     int64         \n",
      " 1   Customer_Id       85 non-null     int64         \n",
      " 2   Amount_GBP        85 non-null     int64         \n",
      " 3   Currency_Route    85 non-null     object        \n",
      " 4   Transaction_Date  85 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(3), object(1)\n",
      "memory usage: 4.0+ KB\n",
      "None\n",
      "\n",
      "Null Values in customer_df:\n",
      "Customer_Id                0\n",
      "Customer_Type              4\n",
      "Current_Address_Country    0\n",
      "Customer_Since_Date        0\n",
      "dtype: int64\n",
      "\n",
      "Null Values in transactions_df:\n",
      "Transaction_Id      0\n",
      "Customer_Id         0\n",
      "Amount_GBP          0\n",
      "Currency_Route      0\n",
      "Transaction_Date    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicates in customer_df: 0\n",
      "Duplicates in transactions_df: 0\n",
      "\n",
      "Unique Values in Customer_Type:\n",
      "['Business' 'Personal' nan]\n",
      "\n",
      "Unique Values in Current_Address_Country:\n",
      "['FRA' 'ISL' 'UK' 'DEU' 'USA' 'PRT' 'AUS' 'GBR' 'ESP' 'NOR' 'LIE' 'ITA']\n",
      "\n",
      "Unique Dates in transactions_df:\n",
      "count          85\n",
      "unique         16\n",
      "top       EUR-GBP\n",
      "freq            7\n",
      "Name: Currency_Route, dtype: object\n",
      "Date format in Customer_Since_Date is valid.\n",
      "Error in date format: 'Transaction Date'\n",
      "\n",
      "Random Sample from customer_df:\n",
      "    Customer_Id Customer_Type Current_Address_Country Customer_Since_Date\n",
      "18           19           NaN                     FRA          2022-08-25\n",
      "20           21      Business                     USA          2023-08-17\n",
      "16           17      Personal                      UK          2023-10-21\n",
      "62           63      Personal                      UK          2022-11-13\n",
      "96           97      Business                     ISL          2023-11-19\n",
      "\n",
      "Random Sample from transactions_df:\n",
      "    Transaction_Id  Customer_Id  Amount_GBP Currency_Route Transaction_Date\n",
      "36              23           51         110        EUR-GBP       2022-07-10\n",
      "11               9           18         235        GBP-EUR       2022-05-02\n",
      "86              38           96         746        EUR-USD       2022-03-06\n",
      "79             126           91         458        USD-EUR       2022-11-30\n",
      "28              28           37         985        EUR-USD       2023-08-20\n",
      "Missing references in transactions_df:    Transaction_Id  Customer_Id  Amount_GBP Currency_Route Transaction_Date\n",
      "5              85            3         460        USD-USD       2022-12-30\n",
      "6             117            4        1000        USD-AUD       2022-08-21\n",
      "\n",
      "Generating report...\n",
      "Report saved to Wise_CS/output/data_quality_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --- Create the output directory if it doesn't exist ---\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "def perform_qa_checks(customer_df, transactions_df):\n",
    "    \"\"\"\n",
    "    Performs basic quality assurance checks on the data.\n",
    "\n",
    "    Args:\n",
    "      customer_df: DataFrame containing customer data.\n",
    "      transactions_df: DataFrame containing transaction data.\n",
    "    \"\"\"\n",
    "    # --- Check the Structure of the DataFrame ---\n",
    "    print(\"Structure of customer_df:\")\n",
    "    print(customer_df.info())\n",
    "    print(\"\\nStructure of transactions_df:\")\n",
    "    print(transactions_df.info())\n",
    "\n",
    "    # --- Check for Null Values ---\n",
    "    print(\"\\nNull Values in customer_df:\")\n",
    "    print(customer_df.isnull().sum())\n",
    "    print(\"\\nNull Values in transactions_df:\")\n",
    "    print(transactions_df.isnull().sum())\n",
    "\n",
    "    # --- Check for Duplicates ---\n",
    "    print(\"\\nDuplicates in customer_df:\", customer_df.duplicated().sum())\n",
    "    print(\"Duplicates in transactions_df:\", transactions_df.duplicated().sum())\n",
    "\n",
    "    # --- Check Ranges and Expected Values ---\n",
    "    print(\"\\nUnique Values in Customer_Type:\")\n",
    "    print(customer_df['Customer_Type'].unique())\n",
    "    print(\"\\nUnique Values in Current_Address_Country:\")\n",
    "    print(customer_df['Current_Address_Country'].unique())\n",
    "    print(\"\\nUnique Dates in transactions_df:\")\n",
    "    print(transactions_df['Currency_Route'].describe())\n",
    "\n",
    "    # --- Validate Date Formats ---\n",
    "    try:\n",
    "        pd.to_datetime(customer_df['Customer_Since_Date'])\n",
    "        print(\"Date format in Customer_Since_Date is valid.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error in date format:\", e)\n",
    "    try:\n",
    "        pd.to_datetime(transactions_df['Transaction Date'])\n",
    "        print(\"Date format in Date is valid.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error in date format:\", e)\n",
    "\n",
    "    # --- Random Sample of Data ---\n",
    "    print(\"\\nRandom Sample from customer_df:\")\n",
    "    print(customer_df.sample(5))\n",
    "    print(\"\\nRandom Sample from transactions_df:\")\n",
    "    print(transactions_df.sample(5))\n",
    "\n",
    "    # --- Check Referential Integrity ---\n",
    "    missing_customers = transactions_df[~transactions_df['Customer_Id'].isin(customer_df['Customer_Id'])]\n",
    "    if missing_customers.empty:\n",
    "        print(\"No broken references between transactions_df and customer_df.\")\n",
    "    else:\n",
    "        print(\"Missing references in transactions_df:\", missing_customers)\n",
    "\n",
    "def generate_report(customer_df, transactions_df):\n",
    "    \"\"\"\n",
    "    Generates a basic data quality report.\n",
    "\n",
    "    Args:\n",
    "      customer_df: DataFrame containing customer data.\n",
    "      transactions_df: DataFrame containing transaction data.\n",
    "\n",
    "    Returns:\n",
    "      A pandas DataFrame containing the data quality report.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Customer Statistics ---\n",
    "    customer_stats = {\n",
    "        ('Customer Statistics', 'Number of Unique Customers'): customer_df['Customer_Id'].nunique(),\n",
    "        ('Customer Statistics', 'Customer Type Distribution'): customer_df['Customer_Type'].value_counts().to_dict(),\n",
    "        ('Customer Statistics', 'Customer Country Distribution'): customer_df['Current_Address_Country'].value_counts().to_dict(),\n",
    "        ('Customer Statistics', 'Customer Since Date Statistics'): customer_df['Customer_Since_Date'].describe().to_dict()\n",
    "    }\n",
    "\n",
    "    # --- Transaction Statistics ---\n",
    "    transaction_stats = {\n",
    "        ('Transaction Statistics', 'Total Transactions'): len(transactions_df),\n",
    "        ('Transaction Statistics', 'Total Amount (GBP)'): transactions_df['Amount_GBP'].sum(),\n",
    "        ('Transaction Statistics', 'Average Transaction Amount (GBP)'): transactions_df['Amount_GBP'].mean(),\n",
    "        ('Transaction Statistics', 'Maximum Transaction Amount (GBP)'): transactions_df['Amount_GBP'].max(),\n",
    "        ('Transaction Statistics', 'Minimum Transaction Amount (GBP)'): transactions_df['Amount_GBP'].min(),\n",
    "        ('Transaction Statistics', 'Transactions by Currency Route'): transactions_df['Currency_Route'].value_counts().to_dict()\n",
    "    }\n",
    "\n",
    "    # --- Combine all stats into a report ---\n",
    "    all_stats = {**customer_stats, **transaction_stats}\n",
    "    report = pd.DataFrame.from_dict(all_stats, orient='index')\n",
    "\n",
    "    return report\n",
    "\n",
    "# --- Run QA checks ---\n",
    "print(\"Running data quality checks...\")\n",
    "perform_qa_checks(customer_df, transactions_df)\n",
    "\n",
    "# --- Generate and display the report ---\n",
    "print(\"\\nGenerating report...\")\n",
    "report = generate_report(customer_df, transactions_df)\n",
    "\n",
    "# --- Save the report to an Excel file ---\n",
    "report.to_excel('output/data_quality_report.xlsx', sheet_name='Data Quality Report', index=True)\n",
    "\n",
    "# --- Load the workbook and select the worksheet ---\n",
    "wb = openpyxl.load_workbook('output/data_quality_report.xlsx')\n",
    "ws = wb['Data Quality Report']\n",
    "\n",
    "# --- Formatting ---\n",
    "# Set the title\n",
    "ws['A1'].value = 'Wise Data Quality Report'\n",
    "ws['A1'].font = Font(bold=True, size=14)\n",
    "\n",
    "# Adjust column widths\n",
    "ws.column_dimensions['A'].width = 30\n",
    "ws.column_dimensions['B'].width = 50\n",
    "\n",
    "# Apply formatting to headers\n",
    "header_font = Font(bold=True)\n",
    "header_border = Border(top=Side(style='thin'), bottom=Side(style='thin'), left=Side(style='thin'), right=Side(style='thin'))\n",
    "header_alignment = Alignment(wrap_text=True, vertical='top')\n",
    "for cell in ws[3]:  # Headers are in row 3\n",
    "    cell.font = header_font\n",
    "    cell.border = header_border\n",
    "    cell.alignment = header_alignment\n",
    "\n",
    "# Apply formatting to the index\n",
    "index_font = Font(bold=True)\n",
    "index_border = Border(top=Side(style='thin'), bottom=Side(style='thin'), left=Side(style='thin'), right=Side(style='thin'))\n",
    "index_alignment = Alignment(wrap_text=True, vertical='top')\n",
    "for cell in ws['A']:\n",
    "    cell.font = index_font\n",
    "    cell.border = index_border\n",
    "    cell.alignment = index_alignment\n",
    "\n",
    "wb.save('output/data_quality_report.xlsx')\n",
    "print(\"Report saved to Wise_CS/output/data_quality_report.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b4ade-231e-453b-982f-1ae64cbd1920",
   "metadata": {},
   "source": [
    "## 5. Save DataFrames <a id='section_5'></a>\n",
    "\n",
    "* [Table of Contents](#section_0)\n",
    "\n",
    "This code saves the customer_df and transactions_df DataFrames as CSV files in the output directory. It then generates dynamic filenames for these CSVs by appending the current month and year, and uploads them to a Google Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34dbe8c4-d54c-49b1-a3e4-2b59cbd0089a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded customer_data.csv to GCS\n",
      "Uploaded transactions_data.csv to GCS\n",
      "Uploaded data_quality_report_2024-10-06.xlsx to GCS\n",
      "Files uploaded successfully to GCS.\n",
      "Starting BigQuery table creation and loading process...\n",
      "Loaded 96 rows into BigQuery table wiseentitydataflow.wise_dataset.customers\n",
      "Loaded 85 rows into BigQuery table wiseentitydataflow.wise_dataset.transactions\n",
      "BigQuery table creation and loading process completed.\n"
     ]
    }
   ],
   "source": [
    "def create_and_load_table(customer_df, transactions_df, project_id, dataset_id, table_id_customer, table_id_transactions):\n",
    "    \"\"\"\n",
    "    Creates and loads tables in BigQuery.\n",
    "\n",
    "    Args:\n",
    "        customer_df: DataFrame containing customer data.\n",
    "        transactions_df: DataFrame containing transaction data.\n",
    "        project_id: Google Cloud project ID.\n",
    "        dataset_id: BigQuery dataset ID.\n",
    "        table_id_customer: BigQuery table ID for customer data.\n",
    "        table_id_transactions: BigQuery table ID for transaction data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Starting BigQuery table creation and loading process...\")\n",
    "\n",
    "        client = bigquery.Client(project=project_id)  # Use project_id variable\n",
    "        dataset_ref = client.dataset(dataset_id)    # Use dataset_id variable\n",
    "\n",
    "        # Create the customer table\n",
    "        table_ref_customer = dataset_ref.table(table_id_customer)\n",
    "        job_config_customer = bigquery.LoadJobConfig(\n",
    "            autodetect=True, \n",
    "            write_disposition=\"WRITE_APPEND\"\n",
    "        )\n",
    "        job_customer = client.load_table_from_dataframe(\n",
    "            customer_df, table_ref_customer, job_config=job_config_customer\n",
    "        )\n",
    "        job_customer.result()  # Wait for the job to complete\n",
    "        print(f\"Loaded {job_customer.output_rows} rows into BigQuery table {project_id}.{dataset_id}.{table_id_customer}\")\n",
    "\n",
    "        # Create the transactions table\n",
    "        table_ref_transactions = dataset_ref.table(table_id_transactions)\n",
    "        job_config_transactions = bigquery.LoadJobConfig(\n",
    "            autodetect=True,\n",
    "            write_disposition=\"WRITE_APPEND\" \n",
    "        )\n",
    "        job_transactions = client.load_table_from_dataframe(\n",
    "            transactions_df, table_ref_transactions, job_config=job_config_transactions\n",
    "        )\n",
    "        job_transactions.result()  # Wait for the job to complete\n",
    "        print(f\"Loaded {job_transactions.output_rows} rows into BigQuery table {project_id}.{dataset_id}.{table_id_transactions}\")\n",
    "\n",
    "        print(\"BigQuery table creation and loading process completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating or loading tables in BigQuery: {e}\")\n",
    "\n",
    "\n",
    "def save_data(customer_df, transactions_df, bucket_name, destination_blob_name_customer, \n",
    "              destination_blob_name_transactions, project_id, dataset_id, \n",
    "              table_id_customer, table_id_transactions):\n",
    "    \"\"\"\n",
    "    Saves the DataFrames as CSV files, uploads them to Google Cloud Storage, \n",
    "    and creates and loads tables in BigQuery. Also uploads a data quality report.\n",
    "\n",
    "    Args:\n",
    "        customer_df: DataFrame containing customer data.\n",
    "        transactions_df: DataFrame containing transaction data.\n",
    "        bucket_name: Name of the GCS bucket.\n",
    "        destination_blob_name_customer: Filename for the customer data on GCS.\n",
    "        destination_blob_name_transactions: Filename for the transaction data on GCS.\n",
    "        project_id: Google Cloud project ID.\n",
    "        dataset_id: BigQuery dataset ID.\n",
    "        table_id_customer: BigQuery table ID for customer data.\n",
    "        table_id_transactions: BigQuery table ID for transaction data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save DataFrames to CSV files \n",
    "        customer_df.to_csv('output/customer_data.csv', index=False)\n",
    "        transactions_df.to_csv('output/transactions_data.csv', index=False)\n",
    "\n",
    "        # Access the bucket\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Upload the files with the dynamic filenames\n",
    "        blob_customer = bucket.blob(destination_blob_name_customer)\n",
    "        blob_customer.upload_from_filename('output/customer_data.csv')\n",
    "        print(f\"Uploaded {blob_customer.name} to GCS\") \n",
    "\n",
    "        blob_transactions = bucket.blob(destination_blob_name_transactions)\n",
    "        blob_transactions.upload_from_filename('output/transactions_data.csv')\n",
    "        print(f\"Uploaded {blob_transactions.name} to GCS\") \n",
    "\n",
    "        # Get today's date for the report filename\n",
    "        today = datetime.date.today().strftime(\"%Y-%m-%d\")  # Corrected line\n",
    "\n",
    "        # Upload the data quality report with today's date in the filename\n",
    "        report_blob_name = f\"data_quality_report_{today}.xlsx\"\n",
    "        report_blob = bucket.blob(report_blob_name)\n",
    "        report_blob.upload_from_filename('output/data_quality_report.xlsx')\n",
    "        print(f\"Uploaded {report_blob.name} to GCS\") \n",
    "\n",
    "        print(\"Files uploaded successfully to GCS.\")\n",
    "\n",
    "        # Create and load tables in BigQuery\n",
    "        create_and_load_table(customer_df, transactions_df, project_id, dataset_id, table_id_customer, table_id_transactions)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving or uploading the data: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Call the save_data function to execute the entire process.\n",
    "    save_data(customer_df, transactions_df, bucket_name, \n",
    "              destination_blob_name_customer, destination_blob_name_transactions, \n",
    "              project_id, dataset_id, table_id_customer, table_id_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ce1e0-cfae-44bf-819c-111b95df9c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
